{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Data processing at scale - Introduction to Spark\n",
    "\n",
    "---\n",
    "\n",
    "**Date:**\n",
    "\n",
    "**Group:**\n",
    " - *Student Name 1*\n",
    " - *Student Name 2*\n",
    "---\n",
    "\n",
    "In this lab, we'll be using the [Spark](http://spark.apache.org) framework to process and analyse some data ([Quick overview](http://spark.apache.org/docs/1.6.2/quick-start.html))\n",
    "\n",
    "The `pyspark` Python module provide the necessary bindings to the Spark engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# General dependencies\n",
    "# !! run this cell first before any other ones\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.rdd\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the Spark Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not globals().get('sc'):\n",
    "    sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look to the functions provided by the SparkContext in this notebook using the `help()` function or [online](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Spark Context is started, you can have access to the UI to visualize some information:\n",
    "http://127.0.0.1:4040/ (change the ip to the correct one if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a random list of integers as a sample of data to perform some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [random.randint(0,10) for i in range(0, 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a RDD from this data using the `parallelize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numSlices` indicates the number of partitions into which the data will be split. Each partition represent a subset of the data on which Spark will apply your transformations/processing in parallel.\n",
    "\n",
    "For instance to create 4 partitions for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd = sc.parallelize(data, 4)\n",
    "print('df type:', type(my_rdd))\n",
    "print('Num partitions:', my_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD also has its own set of functions ([online](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD) documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply count the number of objects in the RDD using the `count()` action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of elements:', my_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you access the UI at http://127.0.0.1:4040/jobs/, you can see that a new job (computation) have been executed and Spark gives some information about it. You can see for instance that 4 tasks were created which correspond to the initial partitioning of the data set.\n",
    "\n",
    "By following the link for this stage (http://127.0.0.1:4040/stages/stage/?id=0&attempt=0), you can also display the execution graph (DAG visualization) and get more metrics about the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `filter` method, we can filter elements in a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to filter elements greater than 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_5 = my_rdd.filter(lambda x: x>=5)\n",
    "greater_than_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the UI (http://127.0.0.1:4040/jobs/), no jobs have been submitted yet. Spark only execute code when an action is requested (see https://spark.apache.org/docs/1.6.2/programming-guide.html#actions for the list of actions).\n",
    "\n",
    "`filter` is a transformation: https://spark.apache.org/docs/1.6.2/programming-guide.html#transformations\n",
    "\n",
    "To query the first 10 elements greater than 5 we can use the `top` action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 first elements greater than 5:', greater_than_5.top(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that a new job has appeared in the UI: http://127.0.0.1:4040/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the number of elements greater than 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of elements greater than 5:', greater_than_5.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than sorting the RDD with `top()`, we can just `take()` the first elements of the RDD (unsorted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_5.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we want to know the distinct values contained in the RDD, we can use the `distinct()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values = greater_than_5.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in order to return all the elements in the new RDD `d_values`, we can use the `collect()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(d_values.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a proper Python function and use it as a filter rather than using lambda function. This is useful when the filtering or processing is more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(value):\n",
    "    return value<5\n",
    "\n",
    "lower_than_5 = my_rdd.filter(my_filter)\n",
    "\n",
    "print('5 last elements lower than 5:', lower_than_5.top(5))\n",
    "print('list of all elements lower than 5:', lower_than_5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data into files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `collect()` returns all the elements and data can be stored in a local Python variable. \n",
    "\n",
    "In case of large data sets, this may be not practical and the result can rather be stored into a file using the `saveAsTextFile` action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.saveAsTextFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('lower_than_5'):\n",
    "    lower_than_5.saveAsTextFile('lower_than_5')\n",
    "else:\n",
    "    print('Directory already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is being written to the `lower_than_5` directory with one file per partition of the RDD (`part-XXXXX`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('lower_than_5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each `.crc` file contain a checksum of the associated file. This is for checking integrity of the files (Cyclical Redundancy Check)\n",
    "* `_SUCCESS` means the operation completed successfully\n",
    "* Data is stored in the `part-*` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 lines of the part-0001 file\n",
    "! head lower_than_5/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of lines for each part-xxxx generated files\n",
    "! wc -l lower_than_5/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map / Reduce\n",
    "We have looked at `filter` that is a Spark transformation, and there are more transformations provided by the Spark API. \n",
    "\n",
    "The `map` transformation allows to apply some processing to all elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to multiply all the elements by 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_by_2 = my_rdd.map(lambda x: x*2)\n",
    "\n",
    "print('Multiply by 2:', multiply_by_2.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reduce function is also available. Let's count the number of occurences of each integer using the `reduceByKey` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is similar to Map/Reduce, for each integer of the RDD we emit a tuple `(int, 1)`, `reduceByKey` will:\n",
    " * group the new RDD by its key (first element of the tuple) \n",
    " * sum up the list of values associated to this key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit a tuple (x, 1) for each record in the original data set\n",
    "values = my_rdd.map(lambda x: (x, 1))\n",
    "\n",
    "print('Example of record in values:', values.take(10))\n",
    "\n",
    "# Group the values by the first element of the tuple (key) and sum all the values (second element of the tuple)\n",
    "group_by_value = values.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "print('Example of record in group_by_value', group_by_value.take(5))\n",
    "\n",
    "# Display sl result\n",
    "print('Group by value:')\n",
    "for (k, v) in group_by_value.collect():\n",
    "    print('\\tvalue %s: count: %s' % (k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External data sets can be loaded, for instance we can load a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sc.textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('lower_than_5/part-00000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines is an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of lines for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of lines:', lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 - Word Count\n",
    "\n",
    "We are considering in this exercice the list of all unique artist terms (Echo Nest tags) from the [Million Song Data Set](http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset).\n",
    "\n",
    "You can download the data set at:\n",
    "\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/unique_terms.txt\n",
    "\n",
    "This is a text file containing a list of tags associated to artits in the Million Song Data Set. There is one tag per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data set\n",
    "!curl -O https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/unique_terms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check/display full path\n",
    "!ls $(pwd)/unique_terms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head $(pwd)/unique_terms.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data set, answer the following questions:\n",
    "\n",
    "**Q:** Create a RDD from this file using the `textFile()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "lines = sc.textFile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Generate a new RDD by transforming each line and splitting it to get all the words (use the [`flatMap` function](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.flatMap)). You can use the `split` function on a Python string to return a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "words = lines.flatMap(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Count the number of words by emitting a key-value pair `(word, 1)` and using the `reduceByKey` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "word_count = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then collect the top 10 by count (look at the [`takeOrdered` function](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.takeOrdered) of a RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.RDD.takeOrdered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in word_count.takeOrdered(10, key=lambda x: -x[1]):\n",
    "    print(\"'%s': %d\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the `wordcloud` library is present in your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous command failed, install the libary by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now display a word cloud of the data sets using the method `fit_words` \n",
    "(see http://amueller.github.io/word_cloud/auto_examples/simple.html as an example)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the result data set into a dictionary word -> count to use with the fit_words method\n",
    "frequencies = {}\n",
    "for el in word_count.collect():\n",
    "    frequencies[el[0]] = el[1]\n",
    "    \n",
    "# build the WordCloude object\n",
    "wc = wordcloud.WordCloud()\n",
    "\n",
    "# build & display the image\n",
    "wc.fit_words(frequencies)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 - Data exploration 1\n",
    " \n",
    "This data set contains the playlist of about 1K lastFM users.\n",
    "\n",
    "See http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html\n",
    "\n",
    "You can download the specific file at:\n",
    "    \n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv.gz\n",
    "    \n",
    "For instance you could run the following shell commands in a terminal:\n",
    "\n",
    "    curl -O https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv.gz\n",
    "    gunzip userid-timestamp-artid-artname-traid-traname.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the testing/validation, run first your code on a smaller subset:\n",
    "\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname-1000.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname-1000.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are confident, you can run the code on the full data set.\n",
    "\n",
    "The format of this file is a TSV (Tab Separated Values) with the following structure:\n",
    "\n",
    "    userid \\t timestamp \\t musicbrainz-artist-id \\t artist-name \\t musicbrainz-track-id \\t track-name\n",
    "    \n",
    "As artist identifiers and track identifiers, you will use the `artist-name` field and the `track-name` fields (the `musicbrain-*` fields may be not provided for all rows).    \n",
    "\n",
    "Using this data set, answer the following questions.\n",
    "\n",
    "**Q:** Load the lastfm data set into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Compute the total number of plays by `artist-name` and display the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Compute the total number of plays by `userid` and display the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Compute the top 20 tracks being played. Hint: Several artists may have created a track of the same name, they are however different tracks and should be counted independently from each others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a RDD containing the words in the tracks names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How many distinct words in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What is the top 10 of most frequent words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Filter out preposition, articles, ... and compute the new top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Display a Word Cloud of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Generate the word cloud for the full data set:\n",
    "        \n",
    "        https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv.gz\n",
    "        \n",
    "If needed, modify the words to be excluded to only keep the one really relevant (eg `in`, `and`, `for`, ... should also be removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 - Collaborative filtering\n",
    "\n",
    "### Introduction\n",
    "https://en.wikipedia.org/wiki/Collaborative_filtering\n",
    "\n",
    "> Collaborative filtering (CF) is a technique used by some recommender systems [...]\n",
    "\n",
    "> [...] collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.\n",
    "\n",
    "For this exercice we'll be using the [Last.fm Dataset - 360K users](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-360K.html) and build a model to recommend new artists to users based on their listening activities. \n",
    "\n",
    "A full data set and a reduced one (for testing) is available at:\n",
    "\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.1000\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.1000    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O  https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "These are TSV files with the following format:\n",
    "\n",
    "    user-mboxsha1 \\t musicbrainz-artist-id \\t artist-name \\t plays\n",
    "\n",
    "The Spark mlib library implements one commonly used algorithm for collaborative filtering analysis based on the Alternating Least Square method (ALS). See http://spark.apache.org/docs/1.6.2/mllib-collaborative-filtering.html for example and details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "To apply the ALS technique on the data set, we first need to convert all values to numerical fields to build a [`Rating` object](https://spark.apache.org/docs/1.6.3/api/python/pyspark.mllib.html?highlight=rating#pyspark.mllib.recommendation.Rating).\n",
    "\n",
    "By consequence, we need to assign unique integer values to users and artists (they currently are respectively SHA-1 and UUIDs - strings -)\n",
    "\n",
    "**Q:** Load the file into a RDD using the `textFile` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "source = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the data set, list all artists and assign them a unique ID.\n",
    "\n",
    "For this:\n",
    " * We write a function that given a line of the file, split it and return the tuple `(musicbrainz-artist-id, artist-name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_artist(line):\n",
    "    data = line.split('\\t')\n",
    "    return (data[1], data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run this function through the data set and get the list of unique artists using the `distinct()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = source.map(get_artist).distinct()\n",
    "print('Number of artists:', artists.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on to train the model, we need to transform the artist into a integer. To achieve this, we can for instance build an index with the [`zipWithUniqueId` function](https://spark.apache.org/docs/1.6.3/api/python/pyspark.html?highlight=zipwithuniqueid#pyspark.RDD.zipWithUniqueId) and use the Spark `broadcat` feature to make it available to all the Spark workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a unique integer ID for each artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_with_unique_id = artists.zipWithUniqueId().collect()\n",
    "artists_with_unique_id[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build an index to map artist to its id which can be used by Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_index = sc.broadcast(dict(zip([a[0] for a in artists_with_unique_id], [a[1] for a in artists_with_unique_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artists_index is a Spark `Broadcast` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(artists_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `value` property of the `Broadcast` object is a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(artists_index.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary contains the mapping artits --> unique integer id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(artists_index.value[('5e7ccd92-6277-451a-aab9-1efd587c50f3', 'steve vai')])\n",
    "print(artists_index.value[('82dc508a-dbda-4954-aedc-28895edfa42e', \"2 many dj's\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to do the same for users:\n",
    "\n",
    "**Q:** Write a function to get the `userid` from a line of the source RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def get_user(line):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a RDD containing all unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "users = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a unique ID for each user using the previous technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a spark broadcast variable to distribute it to the Spark workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to parse the data set and create a new `Rating` object for each data point, this will be \n",
    "```\n",
    "Rating(user_id, artist_id, listens)\n",
    "```\n",
    "where:\n",
    "* `user_id` is the integer id we have assigned\n",
    "* `artist_id` is the integer id of the artist we have assigned\n",
    "* `listens` is the number of listens for this user and artist\n",
    "\n",
    "In order to generate the Ratings:\n",
    "\n",
    "**Q:** Write a function that will parse each line of the source and build the above `Rating` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def extract_rating(line):\n",
    "    ...\n",
    "    return Rating(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Apply the function to the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ratings = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Display the first 5 ratings of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "We need to split the data set in 2 different ones:\n",
    " * training\n",
    " * test\n",
    "\n",
    "To split the data set, we'll be removing random entries in the data set by sampling it. \n",
    "\n",
    "**Q:** Use the `randomSplit()` function on the `ratings` rdd to split the data set into a `training` and `test` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ratings.randomSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "training, test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "We can now run the recommender on this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using Alternating Least Squares with implicit ratings\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "alpha = 0.01\n",
    "lambda_ = 0.01\n",
    "\n",
    "model = ALS.trainImplicit(training, rank, numIterations, alpha=alpha, lambda_=lambda_, nonnegative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trainImplicit` method is going to trigger a set of Spark jobs to train the model. Use the Spark UI to have a look to the different steps generated (http://localhost:4040/jobs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "The generated model can be saved to disk for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(sc, \"myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then loaded when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = MatrixFactorizationModel.load(sc, \"myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recommender` object can now be used to perform some recommendations.\n",
    "\n",
    "**Q:** Pick a random user in the training set and predict the 10 most likely artists he will be interested in (`recommendProducts()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(recommender.recommendProducts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Pick a random artist and predict the 10 most likely users that will be interested by this artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Now that we have all the elements in place, re-run your model against the full data set available at:\n",
    "        \n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/scimus-2017/lab3/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.gz\n",
    "\n",
    "This means:\n",
    " * loading the data into an RDD\n",
    " * splitting it into training and test\n",
    " * creating a new model based on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We have arbitrary chosen some factors for training the model (*lambda*, *rank*, *iterations*, ...).\n",
    "\n",
    "In practice, the recommender has to be evaluated on a data set and tuned according to an evaluation metric.\n",
    "\n",
    "For simplicity we'll evaluate the model on whether or not the first top 10 recommendations are in the users playlist.\n",
    "\n",
    "* For each users, write a function that counts the number of common artists between the user's listen patterns and the recommendations\n",
    "* Compute the average number of common elements on the test data set\n",
    "* Investigate the impact of different parameters on the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
